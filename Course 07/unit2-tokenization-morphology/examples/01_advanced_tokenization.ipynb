{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Unit 2 - Example 1: Advanced Tokenization\n",
        "الوحدة 2 - مثال 1: التقطيع المتقدم\n",
        "\n",
        "This example demonstrates:\n",
        "1. Word tokenization\n",
        "2. Sentence tokenization\n",
        "3. Subword tokenization\n",
        "4. Morphological analysis\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 1: Advanced Tokenization\")\n",
        "print(\"مثال 1: التقطيع المتقدم\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Sample text\n",
        "sample_text = \"\"\"\n",
        "Natural Language Processing is amazing! It helps computers understand human language.\n",
        "We can tokenize text into words, sentences, or even subwords. This is fundamental to NLP.\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nOriginal Text:\")\n",
        "print(\"النص الأصلي:\")\n",
        "print(sample_text)\n",
        "\n",
        "# 1. Word Tokenization\n",
        "# تقطيع الكلمات\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"1. Word Tokenization\")\n",
        "print(\"تقطيع الكلمات\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def word_tokenize(text):\n",
        "    \"\"\"\n",
        "    Simple word tokenization.\n",
        "    تقطيع بسيط للكلمات.\n",
        "    \"\"\"\n",
        "    # Remove punctuation and split\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    words = text.lower().split()\n",
        "    return words\n",
        "\n",
        "words = word_tokenize(sample_text)\n",
        "print(f\"\\nWords: {words}\")\n",
        "print(f\"Total words: {len(words)}\")\n",
        "\n",
        "# Word frequency\n",
        "word_freq = Counter(words)\n",
        "print(\"\\nMost common words:\")\n",
        "print(\"أكثر الكلمات شيوعاً:\")\n",
        "for word, freq in word_freq.most_common(5):\n",
        "    print(f\"  {word}: {freq}\")\n",
        "\n",
        "# 2. Sentence Tokenization\n",
        "# تقطيع الجمل\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2. Sentence Tokenization\")\n",
        "print(\"تقطيع الجمل\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def sentence_tokenize(text):\n",
        "    \"\"\"\n",
        "    Simple sentence tokenization.\n",
        "    تقطيع بسيط للجمل.\n",
        "    \"\"\"\n",
        "    # Split on sentence endings\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return sentences\n",
        "\n",
        "sentences = sentence_tokenize(sample_text)\n",
        "print(f\"\\nSentences: {len(sentences)}\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"  {i}. {sentence}\")\n",
        "\n",
        "# 3. Subword Tokenization\n",
        "# تقطيع الكلمات الفرعية\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3. Subword Tokenization\")\n",
        "print(\"تقطيع الكلمات الفرعية\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def subword_tokenize(word, n=3):\n",
        "    \"\"\"\n",
        "    Create n-gram subwords.\n",
        "    إنشاء كلمات فرعية n-gram.\n",
        "    \"\"\"\n",
        "    subwords = []\n",
        "    for i in range(len(word) - n + 1):\n",
        "        subwords.append(word[i:i+n])\n",
        "    return subwords\n",
        "\n",
        "example_word = \"processing\"\n",
        "subwords = subword_tokenize(example_word, n=3)\n",
        "print(f\"\\nWord: {example_word}\")\n",
        "print(f\"3-gram subwords: {subwords}\")\n",
        "\n",
        "# 4. Morphological Analysis Example\n",
        "# مثال على التحليل الصرفي\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4. Morphological Analysis\")\n",
        "print(\"التحليل الصرفي\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simple stemming example\n",
        "# مثال بسيط على الاشتقاق\n",
        "def simple_stem(word):\n",
        "    \"\"\"\n",
        "    Simple stemming (remove common suffixes).\n",
        "    اشتقاق بسيط (إزالة اللواحق الشائعة).\n",
        "    \"\"\"\n",
        "    suffixes = ['ing', 'ed', 's', 'es', 'ly', 'er', 'est']\n",
        "    for suffix in suffixes:\n",
        "        if word.endswith(suffix):\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "test_words = ['running', 'jumped', 'cats', 'quickly', 'faster']\n",
        "print(\"\\nStemming examples:\")\n",
        "print(\"أمثلة على الاشتقاق:\")\n",
        "for word in test_words:\n",
        "    stem = simple_stem(word)\n",
        "    print(f\"  {word} -> {stem}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Example completed successfully!\")\n",
        "print(\"تم إكمال المثال بنجاح!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nNote: For production, use libraries like NLTK, spaCy, or transformers\")\n",
        "print(\"ملاحظة: للاستخدام الفعلي، استخدم مكتبات مثل NLTK أو spaCy أو transformers\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}