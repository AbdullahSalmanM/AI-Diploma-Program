{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Unit 5 - Example 1: Bias Detection in NLP\n",
        "الوحدة 5 - مثال 1: اكتشاف التحيز في معالجة اللغة الطبيعية\n",
        "\n",
        "This example demonstrates:\n",
        "1. Detecting bias in word associations\n",
        "2. Gender bias in language models\n",
        "3. Mitigation strategies\n",
        "\"\"\"\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Example 1: Bias Detection in NLP\")\n",
        "print(\"مثال 1: اكتشاف التحيز في معالجة اللغة الطبيعية\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Understanding Bias in NLP\n",
        "# فهم التحيز في معالجة اللغة الطبيعية\n",
        "print(\"\\n1. What is Bias in NLP?\")\n",
        "print(\"ما هو التحيز في معالجة اللغة الطبيعية؟\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "bias_explanation = \"\"\"\n",
        "Bias in NLP can manifest as:\n",
        "- Gender bias (associating certain professions with genders)\n",
        "- Racial bias (stereotypical associations)\n",
        "- Cultural bias (favoring certain languages/cultures)\n",
        "- Socioeconomic bias (class-based associations)\n",
        "\n",
        "التحيز في معالجة اللغة الطبيعية يمكن أن يظهر كـ:\n",
        "- تحيز جندري (ربط مهن معينة بالأجناس)\n",
        "- تحيز عرقي (ارتباطات نمطية)\n",
        "- تحيز ثقافي (تفضيل لغات/ثقافات معينة)\n",
        "- تحيز اجتماعي اقتصادي (ارتباطات قائمة على الطبقة)\n",
        "\"\"\"\n",
        "\n",
        "print(bias_explanation)\n",
        "\n",
        "# 2. Example: Gender Bias Detection\n",
        "# مثال: اكتشاف التحيز الجندري\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"2. Gender Bias Example\")\n",
        "print(\"مثال على التحيز الجندري\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Simulated word associations (in real scenario, from word embeddings)\n",
        "# ارتباطات كلمات محاكاة (في السيناريو الحقيقي، من تضمينات الكلمات)\n",
        "professions = [\"doctor\", \"nurse\", \"engineer\", \"teacher\", \"pilot\"]\n",
        "gender_associations = {\n",
        "    \"doctor\": {\"male\": 0.7, \"female\": 0.3},\n",
        "    \"nurse\": {\"male\": 0.2, \"female\": 0.8},\n",
        "    \"engineer\": {\"male\": 0.8, \"female\": 0.2},\n",
        "    \"teacher\": {\"male\": 0.4, \"female\": 0.6},\n",
        "    \"pilot\": {\"male\": 0.9, \"female\": 0.1}\n",
        "}\n",
        "\n",
        "print(\"\\nGender associations with professions:\")\n",
        "print(\"الارتباطات الجندرية مع المهن:\")\n",
        "print(\"Profession | Male | Female | Bias Level\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for profession in professions:\n",
        "    male_score = gender_associations[profession][\"male\"]\n",
        "    female_score = gender_associations[profession][\"female\"]\n",
        "    bias_level = abs(male_score - female_score)\n",
        "    bias_status = \"High\" if bias_level > 0.5 else \"Moderate\" if bias_level > 0.3 else \"Low\"\n",
        "    \n",
        "    print(f\"{profession:10} | {male_score:.2f} | {female_score:.2f} | {bias_status}\")\n",
        "\n",
        "# 3. Bias Mitigation Strategies\n",
        "# استراتيجيات التخفيف من التحيز\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"3. Bias Mitigation Strategies\")\n",
        "print(\"استراتيجيات التخفيف من التحيز\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "mitigation_strategies = [\n",
        "    \"Diverse training data - Ensure balanced representation\",\n",
        "    \"Debiasing algorithms - Remove bias from embeddings\",\n",
        "    \"Regular auditing - Test models for bias regularly\",\n",
        "    \"Transparent documentation - Document known biases\",\n",
        "    \"Fair evaluation metrics - Use fairness-aware metrics\"\n",
        "]\n",
        "\n",
        "print(\"\\nStrategies:\")\n",
        "print(\"الاستراتيجيات:\")\n",
        "for i, strategy in enumerate(mitigation_strategies, 1):\n",
        "    print(f\"  {i}. {strategy}\")\n",
        "\n",
        "# 4. Responsible NLP Checklist\n",
        "# قائمة التحقق من معالجة اللغة الطبيعية المسؤولة\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"4. Responsible NLP Checklist\")\n",
        "print(\"قائمة التحقق من معالجة اللغة الطبيعية المسؤولة\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "checklist = [\n",
        "    \"✓ Test for bias in training data\",\n",
        "    \"✓ Evaluate model on diverse test sets\",\n",
        "    \"✓ Document known limitations\",\n",
        "    \"✓ Provide transparency in model decisions\",\n",
        "    \"✓ Regular bias audits\",\n",
        "    \"✓ Include diverse perspectives in development\"\n",
        "]\n",
        "\n",
        "print(\"\\nChecklist:\")\n",
        "print(\"قائمة التحقق:\")\n",
        "for item in checklist:\n",
        "    print(f\"  {item}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Example completed successfully!\")\n",
        "print(\"تم إكمال المثال بنجاح!\")\n",
        "print(\"=\" * 60)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}